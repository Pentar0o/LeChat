# LeChat üê±

Un chat CLI (interface en ligne de commande) simple, l√©ger et personnalisable pour converser avec un grand mod√®le de langage (LLM) gr√¢ce au framework **MLX** d'Apple.

Ce projet est con√ßu pour tourner localement sur les Mac √©quip√©s d'une puce Apple Silicon (M1, M2, M3...). Il affiche √©galement des statistiques de performance (tokens/seconde) apr√®s chaque r√©ponse.

---

## üöÄ Installation

Avant de lancer `LeChat`, vous devez installer les d√©pendances n√©cessaires.

1.  **Pr√©requis :** Assurez-vous d'avoir **Python 3** et **pip** install√©s sur votre Mac.

2.  **Installation des paquets :** Ouvrez votre terminal et ex√©cutez la commande suivante.

    ```bash
    pip3 install mlx mlx-lm termcolor --break-system-packages
    ```
    * `mlx` & `mlx-lm` : Le framework d'Apple pour l'apprentissage automatique optimis√© pour Apple Silicon.
    * `termcolor` : Une librairie pour colorer le texte et rendre la conversation plus lisible.
    * `--break-system-packages` : Cette option est souvent n√©cessaire sur les versions r√©centes de macOS pour permettre √† `pip` d'installer des paquets.

---

## ‚ñ∂Ô∏è Utilisation

### Lancement simple

Pour d√©marrer une conversation avec les param√®tres par d√©faut, ex√©cutez simplement :

```bash
python3 LeChat.py
```
Cela utilisera le mod√®le `mlx-community/Meta-Llama-3.1-8B-Instruct-4bit` et cherchera un fichier `System_Prompt.txt` pour les instructions de l'IA.

### Options de lancement (avanc√©)

Vous pouvez personnaliser le comportement du chat directement depuis la ligne de commande gr√¢ce aux arguments suivants :

| Argument                 | Description                                                                                              | D√©faut                                             |
| ------------------------ | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------- |
| `--model`                | Nom du mod√®le √† utiliser depuis [Hugging Face MLX Community](https://huggingface.co/mlx-community).        | `mlx-community/Meta-Llama-3.1-8B-Instruct-4bit`    |
| `--system_prompt_file`   | Chemin vers le fichier texte contenant le prompt syst√®me (personnalit√© de l'IA).                         | `System_Prompt.txt`                                |
| `--temperature`          | Contr√¥le la "cr√©ativit√©" des r√©ponses. `0.0` est d√©terministe, `1.0` est tr√®s cr√©atif.                   | `0.1`                                              |
| `--max_tokens`           | Nombre maximum de tokens (mots/syllabes) √† g√©n√©rer pour chaque r√©ponse.                                  | `4096`                                             |

### Exemples de commandes

* **Utiliser un mod√®le plus l√©ger (Mistral 7B) :**
    ```bash
    python3 LeChat.py --model "mlx-community/Mistral-7B-Instruct-v0.2-4bit"
    ```

* **Rendre l'IA plus cr√©ative et utiliser un prompt syst√®me sp√©cifique :**
    ```bash
    python3 LeChat.py --temperature 0.8 --system_prompt_file "Creative_Mode.txt"
    ```

---

## üìù Personnalisation du Prompt Syst√®me

Le "prompt syst√®me" est une instruction initiale qui d√©finit la personnalit√©, le r√¥le ou le contexte de l'IA. C'est le meilleur moyen de guider ses r√©ponses.

1.  Cr√©ez un fichier texte (par exemple `System_Prompt.txt`).
2.  √âcrivez √† l'int√©rieur les instructions pour l'IA.
3.  Lancez le script en utilisant l'argument `--system_prompt_file` si votre fichier a un nom diff√©rent.

#### Exemple de contenu pour `System_Prompt.txt` :

```text
Tu es un expert en histoire de France. Tu r√©ponds de mani√®re concise et pr√©cise, en te concentrant uniquement sur les faits historiques. Tu adoptes un ton acad√©mique.
```

---

## Exit

Pour quitter le chat, tapez `bye` et appuyez sur Entr√©e, ou utilisez le raccourci clavier `Ctrl+C`.
